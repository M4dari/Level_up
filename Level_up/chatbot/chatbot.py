import os
from sentence_transformers import SentenceTransformer
import chromadb
from groq import Groq

# Configura√ß√µes
GROQ_API = "chave "
CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "documentos_vivo"

# Fun√ß√£o para inicializar os componentes
def inicializar_componentes():
    """Inicializa o ChromaDB, embedding model e Groq de forma segura"""
    try:
        # Inicializa o ChromaDB
        chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)
        chroma_collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)
        
        # Verifica se a cole√ß√£o tem documentos
        count = chroma_collection.count()
        if count == 0:
            print(f"‚ö†Ô∏è Cole√ß√£o '{COLLECTION_NAME}' est√° vazia!")
            print("üí° Voc√™ precisa executar o processo de indexa√ß√£o primeiro.")
            print("üí° Execute: python indexar_documentos.py")
        else:
            print(f"‚úÖ Cole√ß√£o '{COLLECTION_NAME}' carregada com {count} documentos")
        
        # Inicializa o modelo de embeddings (modelo menor para ser mais r√°pido)
        print("üîÑ Carregando modelo de embeddings (modelo otimizado)...")
        try:
            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Modelo menor e mais r√°pido
            print("‚úÖ Modelo de embeddings carregado com sucesso!")
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo de embeddings: {str(e)}")
            return None, None, None
        
        # Inicializa o Groq
        print("üîÑ Inicializando Groq...")
        llm = Groq(api_key=GROQ_API)
        print("‚úÖ Groq inicializado!")
        
        print("üéâ Chatbot inicializado com sucesso!")
        
        return chroma_collection, embedding_model, llm
    
    except Exception as e:
        print(f"‚ùå Erro ao inicializar componentes: {str(e)}")
        return None, None, None

# Inicializar componentes globalmente (apenas quando importado)
print("üöÄ Inicializando chatbot...")
chroma_collection, embedding_model, llm = inicializar_componentes()

def chat(question: str, top_k: int = 3) -> str:
    """
    Fun√ß√£o que integra RAG: consulta documentos no Chroma, envia para Groq e retorna a resposta.
    """
    
    # Verificar se os componentes foram inicializados
    if not all([chroma_collection, embedding_model, llm]):
        return "‚ùå Erro: Sistema n√£o inicializado corretamente. Verifique os logs do servidor e certifique-se que o banco tem documentos."
    
    try:
        # 1Ô∏è‚É£ Gera embedding da pergunta
        question_embedding = embedding_model.encode([question]).tolist()
        
        # 2Ô∏è‚É£ Consulta os documentos mais similares no Chroma
        resultados = chroma_collection.query(
            query_embeddings=question_embedding,
            n_results=top_k
        )
        
        # 3Ô∏è‚É£ Extrai os textos dos documentos
        docs_similares = resultados['documents'][0] if resultados['documents'] else []
        
        if not docs_similares:
            return "‚ùå N√£o encontrei documentos relacionados √† sua pergunta. Tente reformular sua pergunta ou verifique se o banco de dados foi indexado corretamente."
        
        # 4Ô∏è‚É£ Monta o prompt para o Groq
        contexto = "\n\n".join([f"Documento {i+1}: {doc}" for i, doc in enumerate(docs_similares)])
        
        prompt = f"""Voc√™ √© um assistente da Vivo que ajuda funcion√°rios com d√∫vidas sobre onboarding e processos internos.

Use os documentos abaixo para responder √† pergunta do usu√°rio de forma clara e √∫til:

{contexto}

Pergunta: {question}

Resposta (seja claro, conciso e √∫til):"""
        
        # 5Ô∏è‚É£ Chama o Groq
        chat_completion = llm.chat.completions.create(
            messages=[
                {
                    "role": "system", 
                    "content": "Voc√™ √© um assistente √∫til da Vivo que responde baseado nos documentos fornecidos. Seja claro, conciso e sempre helpful. Responda em portugu√™s."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            model="llama-3.3-70b-versatile",
            max_tokens=1000,
            temperature=0.7
        )
        
        # 6Ô∏è‚É£ Retorna a resposta gerada
        resposta = chat_completion.choices[0].message.content
        return resposta
        
    except Exception as e:
        error_msg = f"Erro ao processar pergunta: {str(e)}"
        print(f"‚ùå {error_msg}")
        return f"‚ùå Desculpe, ocorreu um erro ao processar sua pergunta. Tente novamente em alguns instantes."

# Fun√ß√£o para testar se o chatbot est√° funcionando
def testar_chatbot():
    """Fun√ß√£o para testar o chatbot"""
    if not all([chroma_collection, embedding_model, llm]):
        print("‚ùå Componentes n√£o inicializados")
        return False
    
    try:
        print("üß™ Testando chatbot...")
        resposta = chat("Como funciona o onboarding?")
        print(f"‚úÖ Resposta do teste: {resposta[:150]}...")
        return True
    except Exception as e:
        print(f"‚ùå Erro no teste: {str(e)}")
        return False

# Executar teste se for chamado diretamente
if __name__ == "__main__":
    print("\n" + "="*50)
    print("üß™ MODO TESTE - Testando chatbot...")
    
    if testar_chatbot():
        print("‚úÖ Chatbot funcionando corretamente!")
    else:
        print("‚ùå Chatbot com problemas!")
        print("\nüí° Poss√≠veis solu√ß√µes:")
        print("1. Execute: python limpar_banco.py")
        print("2. Reindexe seus documentos")
        print("3. Verifique sua API key do Groq")
    
    print("="*50)